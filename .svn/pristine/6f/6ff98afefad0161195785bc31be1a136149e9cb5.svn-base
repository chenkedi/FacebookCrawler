package api.facebook.main;

import java.util.List;

import javax.annotation.Resource;

import org.apache.log4j.Logger;
import org.json.JSONObject;
import org.springframework.stereotype.Controller;

import api.facebook.bean.Posts;
import api.facebook.bean.Seeds;
import api.facebook.dao.PostsDao;
import api.facebook.dao.SeedsDao;
import api.facebook.method.GetPosts;
import api.facebook.util.AppContext;

/**
 * 爬取公众人物发表的帖文
 * @author chenkedi
 *
 */
@Controller
public class PostsInfoCrawler
{
	@Resource
	private PostsDao postsDao;
	@Resource
	private SeedsDao seedsDao;
	private int seedLength=3;
	private int cycle=1;//记录爬虫启动以来轮询的次数
	private static final Logger log = Logger.getLogger(PostsInfoCrawler.class);
	
	
	public static void main(String[] args){
		log.info("正在创建数据库连接和缓冲池...");
	    AppContext.initAppCtx();
	    log.info("数据库连接已连接！缓冲池已建立");
	    
	    PostsInfoCrawler crawler= (PostsInfoCrawler) AppContext.appCtx.getBean(PostsInfoCrawler.class);
	    crawler.run();
	}
	
	/**
	 * 用于爬取历史数据
	 */
	public void run(){
		while(true){
			
			//当还有至少一个种子的历史贴文没有爬取完毕时，next方向的爬行继续

				//查询CrawedPosts为0的seed，表示还未经过第一次遍历
				List<Seeds> seeds=seedsDao.readSeedsForPosts(seedLength,0);//获得需要爬取的种子队列
				
				if(seeds.size()!=0){
					for(Seeds temp : seeds){
						//初始化API的getPosts方法
		    			GetPosts getPosts=new GetPosts();
		    			//调用api得到posts的json数据
		    			JSONObject jsonObject=getPosts.callAPI((temp.getFacebookId()!=null)?temp.getFacebookId():temp.getUserName(),"posts");
		    			//数据抽取，将json转换为bean的格式
		    			List<Posts> postsList=getPosts.dataExtract(jsonObject.getJSONObject("posts"),temp.getSeedsId());//如果出现请求错误，seed可能为空，需要做处理
		    			if(postsList.get(0).getStatus()!="error"){
		    				postsDao.batchInsert(postsList);
		    				log.info(temp.getName()+"的贴文批量插入成功!");
		    				//将翻页的链接写入seeds,此处第一次遍历，previous和next都要跟新
		    				seedsDao.updatePreviousPage(postsList.get(0),temp.getSeedsId());
		    				seedsDao.updateNextPage(postsList.get(0),temp.getSeedsId());
		    				log.info(temp.getName()+"的翻页链接更新成功!");
		    				//将已经爬取过的种子标记值更新为1
		    				String sql="UPDATE seeds set crawed_posts=? WHERE seeds_id=?";
		    				seedsDao.updateCrawed(sql,temp.getSeedsId(),1);
		    				log.info("种子的Crawed_posts爬取状态更新成功!\n\n");
		    			}
		    			else{
		    				log.error(temp.getName()+"的贴文获取失败，继续采集下一个种子！");
		    				log.info("为避免系统问题，睡眠10秒钟\n\n");
		    				try {
								Thread.sleep(10*1000);
							} catch (InterruptedException e) {
								e.printStackTrace();
							}
		    			}
					}
				}else{
					//当种子都经过第一次爬取后，就要切换到翻页链接的爬取，使用深度优先。
					//使用facebook提供的分页链接爬取，返回的json对象外面没有包裹说明变量（如posts等）
					//使用这个翻页链接可以避免数据重复
					
					//查询CrawedPosts为1的值，表示还未经过第二轮，即爬取历史数据
					seeds=seedsDao.readSeedsForPosts(seedLength,1);//获得需要爬取的Crawed_posts为1的种子队列
					if(seeds.size()!=0){
						for(Seeds temp : seeds){						
							//初始化API的getPosts方法
			    			GetPosts getPosts=new GetPosts();
			    			//调用api得到posts的json数据
			    			JSONObject jsonObject=getPosts.callAPI((temp.getUserName()!=null)?temp.getUserName():temp.getFacebookId(),"posts",temp.getPostsNextPage());
			    			//数据抽取，将json转换为bean的格式
			    			List<Posts> postsList=getPosts.dataExtract(jsonObject,temp.getSeedsId());//如果出现请求错误，seed可能为空，需要做处理
			    			
			    			//开始深度优先爬取历史贴文
			    			if(postsList.get(0).getStatus()!="error"){
			    				if(postsList.get(0).getStatus()!="empty"){
			    					postsDao.batchInsert(postsList);
				    				log.info(temp.getName()+"的贴文批量插入成功!");
				    				//将最新的翻页的链接写入seeds，注意，此处只应该更新nextpage链接
				    				seedsDao.updateNextPage(postsList.get(0),temp.getSeedsId());
				    				log.info(temp.getName()+"的Next翻页链接更新成功!\n\n");
				    				
			    				}else{
			    					log.info(temp.getName()+"的历史贴文为空，已采集完毕，将Crawed_post状态更新为-1（历史贴文枯竭）继续采集下一个种子！\n\n");
			    					//将已经爬取过的种子标记值更新为-1,表示历史数据枯竭
				    				String sql="UPDATE seeds set crawed_posts=? WHERE seeds_id=?";
				    				seedsDao.updateCrawed(sql,temp.getSeedsId(),-1);
				    				log.info("种子的Crawed_posts爬取状态更新成功!\n\n");
			    				}		
			    			}else{
			    				log.error(temp.getName()+"的贴文获取失败，继续采集下一个种子！");
			    				log.info("为避免系统问题，睡眠10秒钟\n\n");
			    				try {
									Thread.sleep(10*1000);
								} catch (InterruptedException e) {
									e.printStackTrace();
								}
			    				
			    			}
						}
						
					}else{//为1的种子全部爬完，历史数据采集完毕，crawed_posts现在应该被全部置为-1可以开始向未来数据在线爬取了
						
						seeds=seedsDao.readSeedsForPosts(seedLength,-1);//获得需要爬取的Crawed_posts为1的种子队列
						if(seeds.size()!=0){
							for(Seeds temp : seeds){						
								//初始化API的getPosts方法
				    			GetPosts getPosts=new GetPosts();
				    			//调用api得到posts的json数据
				    			JSONObject jsonObject=getPosts.callAPI((temp.getUserName()!=null)?temp.getUserName():temp.getFacebookId(),"posts",temp.getPostsPreviousPage());
				    			//数据抽取，将json转换为bean的格式
				    			List<Posts> postsList=getPosts.dataExtract(jsonObject,temp.getSeedsId());//如果出现请求错误，seed可能为空，需要做处理
				    			
				    			//开始在线爬取首次遍历以后的贴文
				    			if(postsList.get(0).getStatus()!="error"){
				    				if(postsList.get(0).getStatus()!="empty"){
				    					postsDao.batchInsert(postsList);
					    				log.info(temp.getName()+"的贴文批量插入成功!");
					    				//将最新的翻页的链接写入seeds，注意，此处只应该更新nextpage链接
					    				seedsDao.updatePreviousPage(postsList.get(0),temp.getSeedsId());
					    				log.info(temp.getName()+"的previous翻页链接更新成功!");
					    				String sql="UPDATE seeds set crawed_posts=? WHERE seeds_id=?";
					    				seedsDao.updateCrawed(sql,temp.getSeedsId(),-2);
					    				log.info("种子的Crawed_posts爬取状态更新为-2（已经经过至少一轮未来数据采集）成功!\n\n");
				    				}else{
				    					log.info(temp.getName()+"暂时没有发新贴，将crawed_posts置为-2，继续采集下一个种子！");
				    					//将已经爬取过的种子标记值更新为-2,即使它没有采到数据，防止其很久没有发帖，影响其他活跃种子的采集，表示经过了一次未来数据的遍历
					    				String sql="UPDATE seeds set crawed_posts=? WHERE seeds_id=?";
					    				seedsDao.updateCrawed(sql,temp.getSeedsId(),-2);
					    				log.info("种子的Crawed_posts爬取状态更新为-2（已经经过至少一轮未来数据采集）成功!睡眠15秒\n\n");
					    				try {
											Thread.sleep(15*1000);
										} catch (InterruptedException e) {
											// TODO Auto-generated catch block
											e.printStackTrace();
										}
				    				}		
				    			}else{
				    				log.error(temp.getName()+"的贴文获取失败，继续采集下一个种子！");
				    				log.info("为避免系统问题，睡眠15秒钟\n\n");
				    				try {
										Thread.sleep(15*1000);
									} catch (InterruptedException e) {
										e.printStackTrace();
									}
				    				
				    			}
						}
					}else{//状态为-1的种子已经全部遍历一遍，现在为状态为-2，可以开始重置种子为-1，继续进行未来爬行
						String sql="UPDATE seeds set crawed_posts=?";
						seedsDao.resetCrawed(sql,-1);
	    				log.info("自爬虫启动以来，第” "+cycle+" “次的轮询已完成，Crawed_posts重置为-1成功!\n\n");
	    				log.info("在开始下一轮在线轮询时，考虑到用户发帖的频繁度，睡眠15分钟，也可以防止请求过于频繁");
	    				try {
							Thread.sleep(900*1000);
						} catch (InterruptedException e) {
							// TODO Auto-generated catch block
							e.printStackTrace();
						}
	    				cycle++;
	    				
					}
					
				}
			}		
		}	
	}
}
